"""
Mock Ollama integration test for VITA
This simulates Ollama working to test the application flow
"""

import requests
import json
from typing import Dict, Any

def call_ollama_llm(user_input: str, model: str = "llama2") -> str:
    """
    Mock Ollama LLM call that simulates successful integration
    In real implementation, this would call http://localhost:11434/v1/chat/completions
    """
    
    # Simulate Ollama API call
    url = "http://localhost:11434/v1/chat/completions"
    headers = {
        "Content-Type": "application/json"
    }
    payload = {
        "model": model,
        "stream": False,
        "messages": [{"role": "user", "content": user_input}]
    }
    
    try:
        # In real scenario, this would make the actual request
        # response = requests.post(url, headers=headers, json=payload, timeout=30)
        
        # For testing, return a mock educational response
        if "debug" in user_input.lower() or "error" in user_input.lower():
            mock_response = f"""I can help you debug this code! Here's what I found:

üîç **Analysis**: {user_input[:100]}...

üìù **Suggestions**:
1. Check for syntax errors
2. Verify variable names
3. Test with sample inputs
4. Add error handling

üí° **Educational Tip**: Always test your code with different inputs to ensure it works correctly.

Would you like me to explain any specific part in more detail?

*[Response generated by mock Ollama integration]*"""
        
        elif "concept" in user_input.lower() or "explain" in user_input.lower():
            mock_response = f"""Great question about programming concepts!

üéì **Topic**: {user_input[:50]}...

üìö **Explanation**:
This is a fundamental programming concept that helps you understand how code works. Let me break it down:

‚Ä¢ **Key Point 1**: Basic understanding
‚Ä¢ **Key Point 2**: Practical application  
‚Ä¢ **Key Point 3**: Best practices

üîó **Related Concepts**: You might also want to learn about variables, functions, and control structures.

*[Response generated by mock Ollama integration]*"""
        
        else:
            mock_response = f"""Hello! I'm VITA, your programming assistant powered by Ollama.

I can help you with:
‚Ä¢ üêõ Debugging Python code
‚Ä¢ üìö Explaining programming concepts
‚Ä¢ üí° Providing coding tips and best practices

You asked: "{user_input[:100]}..."

Feel free to upload a Python file for debugging or ask me about any programming concept!

*[Response generated by mock Ollama integration]*"""
        
        return mock_response
        
    except Exception as e:
        return f"‚ö†Ô∏è Error from Ollama: {e}"

def call_local_llm_with_fallback(user_input: str) -> str:
    """
    Enhanced LLM caller that tries Ollama first, then falls back to LM Studio
    """
    
    # Try Ollama first (mock for testing)
    try:
        return call_ollama_llm(user_input)
    except Exception as ollama_error:
        print(f"Ollama failed: {ollama_error}")
        
        # Fallback to original LM Studio
        try:
            import requests
            url = "http://localhost:1234/v1/chat/completions"
            headers = {
                "Content-Type": "application/json",
                "Authorization": "Bearer lm-studio"
            }
            payload = {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "stream": False,
                "messages": [{"role": "user", "content": user_input}]
            }
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]
            
        except Exception as lm_studio_error:
            print(f"LM Studio also failed: {lm_studio_error}")
            return f"‚ö†Ô∏è Both Ollama and LM Studio are unavailable. Mock response: I understand you asked '{user_input[:100]}...' - This would normally be processed by the LLM."

def build_chat_callback(llm_function):
    import asyncio
    async def callback(user_input, user, instance):
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, llm_function, user_input)
        instance.send(response, user="VITA", avatar="üß†", respond=False)
    return callback

# Test the mock integration
if __name__ == "__main__":
    print("Testing mock Ollama integration...")
    
    test_inputs = [
        "Help me debug this Python function",
        "Explain what a for loop is",
        "Hello VITA, can you help me learn Python?"
    ]
    
    for test_input in test_inputs:
        print(f"\n--- Testing: '{test_input}' ---")
        response = call_ollama_llm(test_input)
        print(response)
        print("-" * 50)